{"url": "https://www.reddit.com/r/learnprogramming/comments/9b2p5u/is_ono1_better_than_on_on/", "text": "My solution for a data structures problem to find a number that only exist once in an array is the following which I think is O(n)+O(1) ....please correct me if I am wrong:\n\n&#x200B;\n\n public static int singleNumber(int\\[\\] A) {  \n \n\n  \n HashSet <Integer> single = new HashSet<Integer>();  \n HashSet <Integer> duplicate = new HashSet<Integer>();  \n\n\nfor(int i = 0 ; i <A.length; i++){  \n   \n   \n if(single.contains(A\\[i\\]) == false && duplicate.contains(A\\[i\\])==false){  \nsingle.add(A\\[i\\]);  \n }  \n else if(single.contains(A\\[i\\])==true && duplicate.contains(A\\[i\\]) ==false){  \nsingle.remove(A\\[i\\]);  \nduplicate.add(A\\[i\\]);  \n }  \n else{  \n continue;  \n }  \n   \n   \n }  \n   \n for (Integer element : single) {  \n return element;  \n }  \n   \n return 0;  \n   \n }\n\n&#x200B;\n\nThe recommended Solution which I think is O(n)+O(n) is the following:\n\n&#x200B;\n\n&#x200B;\n\npublic static int singleNumber(int\\[\\] A) {           \n\nHashtable<Integer,Integer> ht = new Hashtable<Integer,Integer>();\n\nint number = 0;\n\n//iterate through array and populate count for each array element\n\nfor(int i=0; i<A.length; i++) {\n\nif(ht.containsKey(A\\[i\\])) {\n\nint val = ht.get(A\\[i\\]);\n\nht.put(A\\[i\\], val+1);\n\n} else {\n\nht.put(A\\[i\\], 1);    \n\n}\n\n}\n\n//iterate Hashtable and find one array element where count = 1;\n\nSet<Integer> keys = ht.keySet();\n\nfor(Integer key: keys){\n\nif(ht.get(key) == 1) {\n\nnumber = (int)key;\n\n}\n\n}\n\nreturn number;\n\n  }\n\n&#x200B;\n\n \n\n&#x200B;\n\n&#x200B;\n\n&#x200B;\n\nWhich one is better?", "score": 1, "comments": [{"body": "Big O notation does not care about looping over members a constant number (e.g. twice). Looping over an array twice would be 2n operations => O(n).\n\nBig O is only concerned with exponential relationships with the number of members in an operation (i.e. n, n\\^2, log(n), e\\^n). Any constants are pretty much ignored since those in the average case are inconsequential relative to n.\n\n You should try and think about this property of Big O now, as it is a big part of why Big O is important (more so than trying to find actual implementations to reduce Big O). Additionally, O(n) + O(1) is also O(n) since again, we are only concerned with exponential relationships to n.", "id": "e4zxiyr", "replies": [{"body": "Yep, both algorithms are exactly equivalent!\n\nI'd also like to add that I think your solution is clever and you should be proud of it. There's more than one way to solve a problem like this, and you found a perfectly valid solution that's equally optimal in terms of speed.\n\n&#x200B;", "id": "e50731f", "replies": []}]}, {"body": "I'm not 100% on the big O notation - but .contains is a linear scan, so it's not O(n) + O(1), or O(n) + O(n). It's more along the lines of O(n^(2)) on the upper bound. \n\n&#x200B;\n\nAs for the code itself, the 2nd solution is cleaner, requires less memory, has less .contains calls, and probably executes faster as a result.", "id": "e4zww7m", "replies": [{"body": "I thought since it's a hashset it wasn't a linear scan rather O(1)", "id": "e4zx3e8", "replies": [{"body": "I'm talking worst case (i.e. upper bound). It performs pretty close to O(1) on the average case, but there's still a scanning component to it.", "id": "e4zxbx2", "replies": [{"body": "A hash set might be O(n) in the worst case, but not because it has a linear scan. It only deviates from the theoretical O(1) if the internal bucket structures need to be resized as members are added to the set.", "id": "e4zywfm", "replies": [{"body": "Hash set insertion can indeed take O(n) time occasionally because of resizing, but its amortized time is still O(1). In this case, since you're using it in an inner loop, it's perfectly reasonable to assume it's O(1).\n\n&#x200B;", "id": "e506evs", "replies": []}]}, {"body": "It's better than average-case, though - hash operations are O(1) amortized time. That means that when you use a HashSet in an algorithm, it's correct to assume that each operation takes O(1) time.\n\nI challenge you to find an algorithms textbook or computer science paper that makes use of a HashSet or HashMap in an algorithm and assumes anything other than O(1) time for insertion, deletion, and lookup. It's just not done, unless the entire focus of the paper is on pathologically bad corner cases.\n\n&#x200B;", "id": "e506poh", "replies": []}]}, {"body": "You're correct.", "id": "e506atv", "replies": []}]}, {"body": "This is wrong. Both theoretically and practically, HashSets have constant-time lookup.\n\nNote that HashSet is not just a basic \"hash table\". It's built on top of a *dynamically growing* hash table that re-hashes when it gets too many elements.\n\n&#x200B;", "id": "e506a8y", "replies": []}, {"body": "Checking a hashtable for membership is constant time O(1). ", "id": "e4zx3u1", "replies": []}]}], "title": "Is O(n)+O(1) better than O(n)+ O(n)"}