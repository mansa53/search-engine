{"url": "https://www.reddit.com/r/learnprogramming/comments/9azjwp/best_way_to_aggregate_multiple_websites_some_with/", "text": "I am looking to get an economic news site, the Fed, some job websites, and my brokage acct data hopefully in one place so i can wake up in the morning and see how things are going, ya know? Bonus points if I can get my gmail in there. I basically want MY portion of the internet in one place. I feel like there has got to be a better way to do this, but am unsure of how. RSS feeds look like a prime candidate but some of the websites I am interested in are private and do not offer access to my data in a feed or through an API. Can I di this? Any pointers are much appreciated!", "score": 2, "comments": [{"body": "Using something like selenium or behat. Selenium seems to be most widely used one with support for a metric buttload of languages. \n\nThose are browser testing frameworks that you can instead code to drive to websites, click buttons, enter text, login, click more buttons, scrape data, send to an API or text file or screenshot. Rinse repeat.\n\nWe use them at work to login to some 300k different merchant, processors, gateways, CRMs and other stuff to gather data in an automated fashion. \n\nDefinitely possible. Actually quite easy if you know anything about coding. Not sure if you are looking to code any bit of it yourself. But it would be the most secure(not letting anyone else use your creds or auth tokens) and customizable way to go about it.", "id": "e50f96v", "replies": [{"body": "Selenium and behat are webscraping libraries correct?", "id": "e50jq55", "replies": [{"body": "They can be indirectly. They are libraries for testing anything that can be utilized via a browser.\n\nWhen you build a script in selenium you are driving an honest-to-god web browser. It can be Chrome, Firefox, and I believe you can still do Internet Explorer if you really wanted to \n\nWhile they're not specifically for \"web scraping\" the language used that drives the browser using the selenium Library can grab information off the page and do what you want with it. One problem with some of the scraping libraries like beautiful soup or other DOM parsers, is they don't execute JavaScript or AJAX type stuff. It's not a problem though if you are using a regular browser to do it all.\n\nYou can use Python PHP JavaScript some .NET ones, there are so many seleniumhq.com I think. So the combination of this thing running the browser and doing everything to get to the pages you want. Then in let's say Python you grab pages source or some data from some elements on the page and using standard python writing it to a database or a screenshot maybe that gets uploaded to a site with an HTML page showing you the screenshit from the X number of things you want in one place.. \n\nYou can literally write a script, hit run, and watch the browser window itself do all the things you just told it to do. But the best part of it all is that you can tell the script to run \"headless\" so the window never shows up, just does it all I'm the background. \n\nNow you can see how this can be taken advantage of. Imagine 5 servers with about 10 scripts at least running in parallel scraping data from hundreds of thousands of user accounts so you can aggregate the data for whatever purpose.\n\n", "id": "e50lzsd", "replies": []}]}]}], "title": "Best way to aggregate multiple websites, some with log-ins?"}